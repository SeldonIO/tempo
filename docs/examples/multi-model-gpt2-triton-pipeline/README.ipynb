{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-spencer",
   "metadata": {},
   "source": [
    "# Tempo GPT2 Triton ONNX Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-founder",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-execution",
   "metadata": {},
   "source": [
    "### Workflow Overview\n",
    "\n",
    "In this example we will be doing the following:\n",
    "* Download & optimize pre-trained artifacts\n",
    "* Deploy GPT2 Model and Test in Docker\n",
    "* Deploy GPT2 Pipeline and Test in Docker\n",
    "* Deploy GPT2 Pipeline & Model to Kuberntes and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050db42",
   "metadata": {},
   "source": [
    "## Download & Optimize pre-trained artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "53e129f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘artifacts/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9769b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./artifacts/gpt2-model\", saved_model=True)\n",
    "tokenizer.save_pretrained(\"./artifacts/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67cc4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/gpt2-onnx-model/gpt2-model/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c77f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./artifacts/gpt2-model/saved_model/1 --opset 11  --output ./artifacts/gpt2-onnx-model/gpt2-model/1/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c1ce",
   "metadata": {},
   "source": [
    "## Deploy GPT2 ONNX Model in Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7af2c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ARTIFACT_FOLDER = os.getcwd() + \"/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d1160770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tempo.serve.metadata import ModelFramework, ModelDataArgs, ModelDataArg\n",
    "from tempo.serve.model import Model\n",
    "from tempo.serve.pipeline import Pipeline, PipelineModels\n",
    "from tempo.serve.utils import pipeline, predictmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18182a77",
   "metadata": {},
   "source": [
    "#### Define as tempo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "560b3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = Model(\n",
    "    name=\"gpt2-model\",\n",
    "    platform=ModelFramework.ONNX,\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-onnx-model\",\n",
    "    uri=\"s3://tempo/gpt2/model\",\n",
    "    # TODO: Simplify without need to add output types if array by default\n",
    "    inputs={},\n",
    "    outputs=(np.ndarray,np.ndarray,),\n",
    "    description=\"GPT-2 ONNX Triton Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ab16a",
   "metadata": {},
   "source": [
    "#### Deploy gpt2 model to docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "49544600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.deploy import deploy_local\n",
    "\n",
    "remote_gpt2_model = deploy_local(gpt2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b2962",
   "metadata": {},
   "source": [
    "#### Send predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3cee09f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids:0': array([[1212,  318,  257, 1332]], dtype=int32), 'attention_mask:0': array([[1, 1, 1, 1]], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"This is a test\", return_tensors=\"tf\")\n",
    "attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "gpt2_inputs = {\n",
    "    \"input_ids:0\": input_ids.numpy(),\n",
    "    \"attention_mask:0\": attention_mask\n",
    "}\n",
    "\n",
    "print(gpt2_inputs)\n",
    "\n",
    "gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e3e90",
   "metadata": {},
   "source": [
    "#### Print single next token generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0baae2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "# take the best next token probability of the last token of input ( greedy approach)\n",
    "next_token = logits.argmax(axis=2)[0]\n",
    "next_token_str = tokenizer.decode(\n",
    "    next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ").strip()\n",
    "\n",
    "print(next_token_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41868e11",
   "metadata": {},
   "source": [
    "## Define Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "secure-perth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    name=\"gpt2-transformer\",\n",
    "    uri=\"s3://tempo/gpt2/transformer\",\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-transformer/\",\n",
    "    models=PipelineModels(gpt2_model=gpt2_model),\n",
    "    description=\"A pipeline to use either an sklearn or xgboost model for Iris classification\",\n",
    ")\n",
    "class GPT2Transformer:\n",
    "    # TODO: Set ready = false in init to avoid having to set it\n",
    "    def __init__(self):\n",
    "        self.ready = False\n",
    "        \n",
    "    # TODO: Bug - Pipeline locally doesn't call the load function (expected?)\n",
    "    # TODO: Update load function to change ready to true by default\n",
    "    def load(self, tokenizer_path=\"/mnt/models/\"):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)       \n",
    "        self.ready = True\n",
    "\n",
    "    @predictmethod\n",
    "    def predict(self, payload: np.ndarray) -> np.ndarray:\n",
    "        count = 0\n",
    "        # TODO: Update to allow this to be passed as parameters\n",
    "        max_gen_len = 10\n",
    "        # TODO: Update to work for multiple sentences\n",
    "        gen_sentence = payload[0]\n",
    "        while count < max_gen_len:\n",
    "            input_ids = self.tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "            attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "            gpt2_inputs = {\n",
    "                \"input_ids:0\": input_ids.numpy(),\n",
    "                \"attention_mask:0\": attention_mask\n",
    "            }\n",
    "\n",
    "            gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)\n",
    "\n",
    "            logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "            # take the best next token probability of the last token of input ( greedy approach)\n",
    "            next_token = logits.argmax(axis=2)[0]\n",
    "            next_token_str = tokenizer.decode(\n",
    "                next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            ).strip()\n",
    "            \n",
    "            gen_sentence += \" \" + next_token_str\n",
    "            count += 1\n",
    "        \n",
    "        return gen_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba230538",
   "metadata": {},
   "source": [
    "#### Test locally against deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b3f8b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_transformer = GPT2Transformer()\n",
    "# Load locally manually\n",
    "gpt2_transformer.load(tokenizer_path=ARTIFACT_FOLDER + \"/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4a40468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Setting context to context for insights manager\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_output = gpt2_transformer.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ab03e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love artificial intelligence , but I 'm not sure if it 's worth\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-collective",
   "metadata": {},
   "source": [
    "## Deploy GPT2 Transformer to Docker and Test\n",
    "\n",
    " * In preparation for running our models we save the Python environment needed for the orchestration to run as defined by a `conda.yaml` in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ambient-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting artifacts/gpt2-transformer/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifacts/gpt2-transformer/conda.yaml\n",
    "name: tempo-gpt2\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.7.10\n",
    "  - pip:\n",
    "    - transformers==4.5.1\n",
    "    - tokenizers==0.10.3\n",
    "    - mlserver\n",
    "    - mlserver-tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1cf5f",
   "metadata": {},
   "source": [
    "#### Save environment and pipeline artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fixed-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n",
      "INFO:tempo:Saving environment\n",
      "INFO:tempo:Saving tempo model to /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/model.pickle\n",
      "INFO:tempo:Using found conda.yaml\n",
      "INFO:tempo:Creating conda env with: conda env create --name tempo-40d229b4-4ca9-405a-80a1-d336b174add1 --file /tmp/tmpwm6mf7mo.yml\n",
      "INFO:tempo:packing conda environment from tempo-40d229b4-4ca9-405a-80a1-d336b174add1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packages...\n",
      "Packing environment at '/home/alejandro/miniconda3/envs/tempo-40d229b4-4ca9-405a-80a1-d336b174add1' to '/home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/environment.tar.gz'\n",
      "[########################################] | 100% Completed | 16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Removing conda env with: conda remove --name tempo-40d229b4-4ca9-405a-80a1-d336b174add1 --all --yes\n"
     ]
    }
   ],
   "source": [
    "from tempo.serve.loader import save\n",
    "save(gpt2_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-spanking",
   "metadata": {},
   "source": [
    "#### Deploy locally on Docker\n",
    "\n",
    " * Here we test our models using production images but running locally on Docker. This allows us to ensure the final production deployed model will behave as expected when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "artificial-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_local\n",
    "remote_pipeline = deploy_local(gpt2_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "found-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-oxygen",
   "metadata": {},
   "source": [
    "## Deploy to Kubernetes\n",
    "\n",
    " * Here we illustrate how to run the final models in \"production\" on Kubernetes by using Tempo to deploy\n",
    " \n",
    "### Prerequisites\n",
    " \n",
    "Create a Kind Kubernetes cluster with Minio and Seldon Core installed using Ansible as described [here](https://tempo.readthedocs.io/en/latest/overview/quickstart.html#kubernetes-cluster-with-seldon-core)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collect-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-secret created\r\n",
      "serviceaccount/tempo-pipeline created\r\n",
      "role.rbac.authorization.k8s.io/tempo-pipeline created\r\n",
      "rolebinding.rbac.authorization.k8s.io/tempo-pipeline-rolebinding created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f k8s/rbac -n production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaning-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.examples.minio import create_minio_rclone\n",
    "import os\n",
    "create_minio_rclone(os.getcwd()+\"/rclone.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collect-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.loader import upload\n",
    "upload(gpt2_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "civil-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.metadata import SeldonCoreOptions\n",
    "runtime_options = SeldonCoreOptions(**{\n",
    "        \"remote_options\": {\n",
    "            \"namespace\": \"production\",\n",
    "            \"authSecretName\": \"minio-secret\"\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "binary-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_remote\n",
    "remote_model = deploy_remote(gpt2_pipeline, options=runtime_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blind-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
