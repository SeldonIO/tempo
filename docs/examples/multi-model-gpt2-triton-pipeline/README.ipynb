{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-spencer",
   "metadata": {},
   "source": [
    "# Tempo GPT2 Triton ONNX Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-founder",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-execution",
   "metadata": {},
   "source": [
    "## Create Tempo Artifacts\n",
    "\n",
    " * Here we create the Tempo models and orchestration Pipeline for our final service using our models.\n",
    " * For illustration the final service will call the sklearn model and based on the result will decide to return that prediction or call the xgboost model and return that prediction instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee67f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘artifacts/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9769b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2804262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./artifacts/gpt2-model\", saved_model=True)\n",
    "tokenizer.save_pretrained(\"./artifacts/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "60512586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/gpt2-onnx-model/gpt2-model/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad174e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./artifacts/gpt2-model/saved_model/1 --opset 11  --output ./artifacts/gpt2-onnx-model/gpt2-model/1/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a153045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ARTIFACT_FOLDER = os.getcwd() + \"/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a972f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tempo.serve.metadata import ModelFramework, ModelDataArgs, ModelDataArg\n",
    "from tempo.serve.model import Model\n",
    "from tempo.serve.pipeline import Pipeline, PipelineModels\n",
    "from tempo.serve.utils import pipeline, predictmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0f7352d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = Model(\n",
    "    name=\"gpt2-model\",\n",
    "    platform=ModelFramework.ONNX,\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-onnx-model\",\n",
    "    uri=\"s3://tempo/gpt2/model\",\n",
    "    # TODO: Simplify without need to add output types if array by default\n",
    "    inputs={},\n",
    "    outputs=(np.ndarray,np.ndarray,),\n",
    "    description=\"GPT-2 ONNX Triton Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78073167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.deploy import deploy_local\n",
    "\n",
    "remote_gpt2_model = deploy_local(gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "21905f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids:0': array([[1212,  318,  257, 1332]], dtype=int32), 'attention_mask:0': array([[1, 1, 1, 1]], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"This is a test\", return_tensors=\"tf\")\n",
    "attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "gpt2_inputs = {\n",
    "    \"input_ids:0\": input_ids.numpy(),\n",
    "    \"attention_mask:0\": attention_mask\n",
    "}\n",
    "\n",
    "print(gpt2_inputs)\n",
    "\n",
    "gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b79d81ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "# take the best next token probability of the last token of input ( greedy approach)\n",
    "next_token = logits.argmax(axis=2)[0]\n",
    "next_token_str = tokenizer.decode(\n",
    "    next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ").strip()\n",
    "\n",
    "print(next_token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "secure-perth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    name=\"gpt2-transformer\",\n",
    "    uri=\"s3://tempo/gpt2/transformer\",\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-transformer/\",\n",
    "    models=PipelineModels(gpt2_model=gpt2_model),\n",
    "    description=\"A pipeline to use either an sklearn or xgboost model for Iris classification\",\n",
    ")\n",
    "class GPT2Transformer:\n",
    "    # TODO: Set ready = false in init to avoid having to set it\n",
    "    def __init__(self):\n",
    "        self.ready = False\n",
    "        \n",
    "    # TODO: Bug - Pipeline locally doesn't call the load function (expected?)\n",
    "    # TODO: Update load function to change ready to true by default\n",
    "    def load(self, tokenizer_path=\"/mnt/models/\"):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)       \n",
    "        self.ready = True\n",
    "\n",
    "    @predictmethod\n",
    "    def predict(self, payload: np.ndarray) -> np.ndarray:\n",
    "        count = 0\n",
    "        # TODO: Update to allow this to be passed as parameters\n",
    "        max_gen_len = 10\n",
    "        # TODO: Update to work for multiple sentences\n",
    "        gen_sentence = payload[0]\n",
    "        while count < max_gen_len:\n",
    "            input_ids = self.tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "            attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "            gpt2_inputs = {\n",
    "                \"input_ids:0\": input_ids.numpy(),\n",
    "                \"attention_mask:0\": attention_mask\n",
    "            }\n",
    "\n",
    "            gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)\n",
    "\n",
    "            logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "            # take the best next token probability of the last token of input ( greedy approach)\n",
    "            next_token = logits.argmax(axis=2)[0]\n",
    "            next_token_str = tokenizer.decode(\n",
    "                next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            ).strip()\n",
    "            \n",
    "            gen_sentence += \" \" + next_token_str\n",
    "            count += 1\n",
    "        \n",
    "        return gen_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8d688576",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_transformer = GPT2Transformer()\n",
    "# Load locally manually\n",
    "gpt2_transformer.load(tokenizer_path=ARTIFACT_FOLDER + \"/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "13b24f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Setting context to context for insights manager\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_output = gpt2_transformer.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "13fba362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love artificial intelligence , but I 'm not sure if it 's worth\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-collective",
   "metadata": {},
   "source": [
    "## Save Classifier Environment\n",
    "\n",
    " * In preparation for running our models we save the Python environment needed for the orchestration to run as defined by a `conda.yaml` in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ambient-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting artifacts/gpt2-transformer/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifacts/gpt2-transformer/conda.yaml\n",
    "name: tempo-gpt2\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.7.10\n",
    "  - pip:\n",
    "    - transformers==4.5.1\n",
    "    - tokenizers==0.10.3\n",
    "    - mlserver\n",
    "    - mlserver-tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n",
      "INFO:tempo:Saving environment\n",
      "INFO:tempo:Saving tempo model to /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/model.pickle\n",
      "INFO:tempo:Using found conda.yaml\n",
      "INFO:tempo:Creating conda env with: conda env create --name tempo-40d229b4-4ca9-405a-80a1-d336b174add1 --file /tmp/tmpwm6mf7mo.yml\n"
     ]
    }
   ],
   "source": [
    "from tempo.serve.loader import save\n",
    "save(gpt2_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-spanking",
   "metadata": {},
   "source": [
    "## Test Locally on Docker\n",
    "\n",
    " * Here we test our models using production images but running locally on Docker. This allows us to ensure the final production deployed model will behave as expected when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "artificial-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_local\n",
    "remote_pipeline = deploy_local(gpt2_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "found-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-oxygen",
   "metadata": {},
   "source": [
    "## Production Option 1 (Deploy to Kubernetes with Tempo)\n",
    "\n",
    " * Here we illustrate how to run the final models in \"production\" on Kubernetes by using Tempo to deploy\n",
    " \n",
    "### Prerequisites\n",
    " \n",
    "Create a Kind Kubernetes cluster with Minio and Seldon Core installed using Ansible as described [here](https://tempo.readthedocs.io/en/latest/overview/quickstart.html#kubernetes-cluster-with-seldon-core)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collect-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-secret created\r\n",
      "serviceaccount/tempo-pipeline created\r\n",
      "role.rbac.authorization.k8s.io/tempo-pipeline created\r\n",
      "rolebinding.rbac.authorization.k8s.io/tempo-pipeline-rolebinding created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f k8s/rbac -n production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaning-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.examples.minio import create_minio_rclone\n",
    "import os\n",
    "create_minio_rclone(os.getcwd()+\"/rclone.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collect-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.loader import upload\n",
    "upload(gpt2_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "civil-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.metadata import SeldonCoreOptions\n",
    "runtime_options = SeldonCoreOptions(**{\n",
    "        \"remote_options\": {\n",
    "            \"namespace\": \"production\",\n",
    "            \"authSecretName\": \"minio-secret\"\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "binary-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_remote\n",
    "remote_model = deploy_remote(gpt2_pipeline, options=runtime_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "psychological-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output0': array([1.], dtype=float32), 'output1': 'sklearn prediction'}\n",
      "{'output0': array([[0.00847207, 0.03168793, 0.95984   ]], dtype=float32), 'output1': 'xgboost prediction'}\n"
     ]
    }
   ],
   "source": [
    "remote_model.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blind-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
