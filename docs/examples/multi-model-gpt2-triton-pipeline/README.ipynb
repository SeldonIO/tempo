{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-spencer",
   "metadata": {},
   "source": [
    "# Tempo GPT2 Triton ONNX Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-founder",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-execution",
   "metadata": {},
   "source": [
    "### Workflow Overview\n",
    "\n",
    "In this example we will be doing the following:\n",
    "* Download & optimize pre-trained artifacts\n",
    "* Deploy GPT2 Model and Test in Docker\n",
    "* Deploy GPT2 Pipeline and Test in Docker\n",
    "* Deploy GPT2 Pipeline & Model to Kuberntes and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea872d1",
   "metadata": {},
   "source": [
    "## Download & Optimize pre-trained artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e89392ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘artifacts/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9769b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./artifacts/gpt2-model\", saved_model=True)\n",
    "tokenizer.save_pretrained(\"./artifacts/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1082ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/gpt2-onnx-model/gpt2-model/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./artifacts/gpt2-model/saved_model/1 --opset 11  --output ./artifacts/gpt2-onnx-model/gpt2-model/1/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305bc6d",
   "metadata": {},
   "source": [
    "## Deploy GPT2 ONNX Model in Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f63ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ARTIFACT_FOLDER = os.getcwd() + \"/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15512696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tempo.serve.metadata import ModelFramework, ModelDataArgs, ModelDataArg\n",
    "from tempo.serve.model import Model\n",
    "from tempo.serve.pipeline import Pipeline, PipelineModels\n",
    "from tempo.serve.utils import pipeline, predictmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698ac49",
   "metadata": {},
   "source": [
    "#### Define as tempo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "49381e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = Model(\n",
    "    name=\"gpt2-model\",\n",
    "    platform=ModelFramework.ONNX,\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-onnx-model\",\n",
    "    uri=\"s3://tempo/gpt2/model\",\n",
    "    # TODO: Simplify without need to add output types if array by default\n",
    "    # TODO: Create a doc page that explains inputs\n",
    "    inputs={},\n",
    "    outputs=(np.ndarray,np.ndarray,),\n",
    "    description=\"GPT-2 ONNX Triton Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe83780",
   "metadata": {},
   "source": [
    "#### Deploy gpt2 model to docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c15f42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.deploy import deploy_local\n",
    "\n",
    "remote_gpt2_model = deploy_local(gpt2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684edda",
   "metadata": {},
   "source": [
    "#### Send predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "34f85261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids:0': array([[1212,  318,  257, 1332]], dtype=int32), 'attention_mask:0': array([[1, 1, 1, 1]], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 5360795\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"This is a test\", return_tensors=\"tf\")\n",
    "attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "gpt2_inputs = {\n",
    "    \"input_ids:0\": input_ids.numpy(),\n",
    "    \"attention_mask:0\": attention_mask\n",
    "}\n",
    "\n",
    "print(gpt2_inputs)\n",
    "\n",
    "gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2500300",
   "metadata": {},
   "source": [
    "#### Print single next token generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "795c55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "# take the best next token probability of the last token of input ( greedy approach)\n",
    "next_token = logits.argmax(axis=2)[0]\n",
    "next_token_str = tokenizer.decode(\n",
    "    next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ").strip()\n",
    "\n",
    "print(next_token_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd1a7e",
   "metadata": {},
   "source": [
    "## Define Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "secure-perth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    name=\"gpt2-transformer\",\n",
    "    uri=\"s3://tempo/gpt2/transformer\",\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-transformer/\",\n",
    "    models=PipelineModels(gpt2_model=gpt2_model),\n",
    "    description=\"A pipeline to use either an sklearn or xgboost model for Iris classification\",\n",
    ")\n",
    "class GPT2Transformer:\n",
    "    # TODO: Set ready = false in init to avoid having to set it\n",
    "    def __init__(self):\n",
    "        self.ready = False\n",
    "        \n",
    "    # TODO: Bug - Pipeline locally doesn't call the load function (expected?)\n",
    "    # TODO: Update load function to change ready to true by default\n",
    "    def load(self, tokenizer_path=\"/mnt/models/\"):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)       \n",
    "        self.ready = True\n",
    "\n",
    "    @predictmethod\n",
    "    def predict(self, payload: np.array) -> np.array:\n",
    "        count = 0\n",
    "        # TODO: Update to allow this to be passed as parameters\n",
    "        max_gen_len = 10\n",
    "        # TODO: Update to work for multiple sentences\n",
    "        gen_sentence = payload\n",
    "        while count < max_gen_len:\n",
    "            input_ids = self.tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "            attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "            gpt2_inputs = {\n",
    "                \"input_ids:0\": input_ids.numpy(),\n",
    "                \"attention_mask:0\": attention_mask\n",
    "            }\n",
    "\n",
    "            gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)\n",
    "\n",
    "            logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "            # take the best next token probability of the last token of input ( greedy approach)\n",
    "            next_token = logits.argmax(axis=2)[0]\n",
    "            next_token_str = tokenizer.decode(\n",
    "                next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            ).strip()\n",
    "            \n",
    "            gen_sentence += \" \" + next_token_str\n",
    "            count += 1\n",
    "        \n",
    "        return gen_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40315db6",
   "metadata": {},
   "source": [
    "#### Test locally against deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "cce2979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_transformer = GPT2Transformer()\n",
    "# Load locally manually\n",
    "gpt2_transformer.load(tokenizer_path=ARTIFACT_FOLDER + \"/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6c140c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Setting context to context for insights manager\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 5296925\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 6621721\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 7965054\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 9308014\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 12004625\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 13347162\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 14689468\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 16044702\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 17399133\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:39609/v2/models/gpt2-model/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:39609\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:39609 \"POST /v2/models/gpt2-model/infer HTTP/1.1\" 200 20083773\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_output = gpt2_transformer.predict(\"I love artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "9192b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love artificial intelligence , but I 'm not sure if it 's worth\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-collective",
   "metadata": {},
   "source": [
    "## Deploy GPT2 Transformer to Docker and Test\n",
    "\n",
    " * In preparation for running our models we save the Python environment needed for the orchestration to run as defined by a `conda.yaml` in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ambient-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting artifacts/gpt2-transformer/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifacts/gpt2-transformer/conda.yaml\n",
    "name: tempo-gpt2\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.7.10\n",
    "  - pip:\n",
    "    - transformers==4.5.1\n",
    "    - tokenizers==0.10.3\n",
    "    - tensorflow==2.4.1\n",
    "    - dill\n",
    "    - mlops-tempo\n",
    "    - mlserver\n",
    "    - mlserver-tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a86c2",
   "metadata": {},
   "source": [
    "#### Save environment and pipeline artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "fixed-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n",
      "INFO:tempo:Saving environment\n",
      "INFO:tempo:Saving tempo model to /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/model.pickle\n"
     ]
    }
   ],
   "source": [
    "from tempo.serve.loader import save\n",
    "save(gpt2_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-spanking",
   "metadata": {},
   "source": [
    "#### Deploy locally on Docker\n",
    "\n",
    " * Here we test our models using production images but running locally on Docker. This allows us to ensure the final production deployed model will behave as expected when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "artificial-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-transformer/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-transformer/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-transformer/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-model/json HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "from tempo import deploy_local\n",
    "remote_pipeline = deploy_local(gpt2_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a4e3e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger('urllib3')\n",
    "log.setLevel(logging.DEBUG) \n",
    "logg = logging.getLogger('requests.packages.urllib3')\n",
    "logg.setLevel(logging.DEBUG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "collective-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tempo:Using remote class tempo.seldon.SeldonDockerRuntime\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/gpt2-transformer/json HTTP/1.1\" 200 None\n",
      "DEBUG:tempo:Calling requests POST with endpoint=http://0.0.0.0:57937/v2/models/gpt2-transformer/infer headers={} verify=True\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 0.0.0.0:57937\n",
      "DEBUG:urllib3.connectionpool:http://0.0.0.0:57937 \"POST /v2/models/gpt2-transformer/infer HTTP/1.1\" 200 455\n",
      "DEBUG:tempo:b'{\"model_name\":\"gpt2-transformer\",\"model_version\":\"NOTIMPLEMENTED\",\"id\":\"debcb373-6b87-4c2f-8ed4-47b1d2f45931\",\"parameters\":null,\"outputs\":[{\"name\":\"output0\",\"shape\":[65],\"datatype\":\"BYTES\",\"parameters\":null,\"data\":[73,32,108,111,118,101,32,97,114,116,105,102,105,99,105,97,108,32,105,110,116,101,108,108,105,103,101,110,99,101,32,44,32,98,117,116,32,73,32,39,109,32,110,111,116,32,115,117,114,101,32,105,102,32,105,116,32,39,115,32,119,111,114,116,104]}]}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I love artificial intelligence , but I 'm not sure if it 's worth\""
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_pipeline.predict(\"I love artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "found-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-oxygen",
   "metadata": {},
   "source": [
    "## Deploy to Kubernetes\n",
    "\n",
    " * Here we illustrate how to run the final models in \"production\" on Kubernetes by using Tempo to deploy\n",
    " \n",
    "### Prerequisites\n",
    " \n",
    "Create a Kind Kubernetes cluster with Minio and Seldon Core installed using Ansible as described [here](https://tempo.readthedocs.io/en/latest/overview/quickstart.html#kubernetes-cluster-with-seldon-core)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collect-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-secret created\r\n",
      "serviceaccount/tempo-pipeline created\r\n",
      "role.rbac.authorization.k8s.io/tempo-pipeline created\r\n",
      "rolebinding.rbac.authorization.k8s.io/tempo-pipeline-rolebinding created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f k8s/rbac -n production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaning-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.examples.minio import create_minio_rclone\n",
    "import os\n",
    "create_minio_rclone(os.getcwd()+\"/rclone.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collect-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.loader import upload\n",
    "upload(gpt2_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "civil-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.metadata import SeldonCoreOptions\n",
    "runtime_options = SeldonCoreOptions(**{\n",
    "        \"remote_options\": {\n",
    "            \"namespace\": \"production\",\n",
    "            \"authSecretName\": \"minio-secret\"\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "binary-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_remote\n",
    "remote_model = deploy_remote(gpt2_pipeline, options=runtime_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.predict([\"I love artificial intelligence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blind-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
