{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-spencer",
   "metadata": {},
   "source": [
    "# Tempo GPT2 Triton ONNX Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-founder",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-execution",
   "metadata": {},
   "source": [
    "### Workflow Overview\n",
    "\n",
    "In this example we will be doing the following:\n",
    "* Download & optimize pre-trained artifacts\n",
    "* Deploy GPT2 Model and Test in Docker\n",
    "* Deploy GPT2 Pipeline and Test in Docker\n",
    "* Deploy GPT2 Pipeline & Model to Kuberntes and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea872d1",
   "metadata": {},
   "source": [
    "## Download & Optimize pre-trained artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e89392ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘artifacts/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd9769b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc50ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7faebec9aad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7faebec9aad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./artifacts/gpt2-model/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./artifacts/gpt2-model/saved_model/1/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./artifacts/gpt2-transformer/tokenizer_config.json',\n",
       " './artifacts/gpt2-transformer/special_tokens_map.json',\n",
       " './artifacts/gpt2-transformer/vocab.json',\n",
       " './artifacts/gpt2-transformer/merges.txt',\n",
       " './artifacts/gpt2-transformer/added_tokens.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./artifacts/gpt2-model\", saved_model=True)\n",
    "tokenizer.save_pretrained(\"./artifacts/gpt2-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1082ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/gpt2-onnx-model/gpt2-model/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3df2dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-07 08:43:11.186716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/alejandro/miniconda3/lib/python3.7/runpy.py:125: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-09-07 08:43:12.886148: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-07 08:43:12.886345: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-07 08:43:12.886376: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-07 08:43:12.886392: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-CSLUJOT): /proc/driver/nvidia/version does not exist\n",
      "2021-09-07 08:43:12.888970: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-07 08:43:12,892 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2021-09-07 08:43:19,256 - INFO - Signatures found in model: [serving_default].\n",
      "2021-09-07 08:43:19,256 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2021-09-07 08:43:19,256 - INFO - Output names: ['logits', 'past_key_values']\n",
      "2021-09-07 08:43:19.306853: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-09-07 08:43:19.307167: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-09-07 08:43:19.307630: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-07 08:43:19.316142: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400005000 Hz\n",
      "2021-09-07 08:43:19.501937: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 3213 nodes (3060), 4128 edges (3974), time = 99.635ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 1.408ms.\n",
      "\n",
      "2021-09-07 08:43:27.473999: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "WARNING:tensorflow:From /home/alejandro/miniconda3/lib/python3.7/site-packages/tf2onnx/tf_loader.py:603: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2021-09-07 08:43:29,277 - WARNING - From /home/alejandro/miniconda3/lib/python3.7/site-packages/tf2onnx/tf_loader.py:603: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2021-09-07 08:43:29.353446: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-09-07 08:43:29.353640: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-09-07 08:43:29.353974: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-07 08:43:36.123024: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 2720 nodes (-318), 3646 edges (-319), time = 4489.73486ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 2.24ms.\n",
      "  constant_folding: Graph size after: 2720 nodes (0), 3646 edges (0), time = 1504.76294ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 13.628ms.\n",
      "\n",
      "2021-09-07 08:43:39,251 - INFO - Using tensorflow=2.4.0, onnx=1.9.0, tf2onnx=1.8.5/50049d\n",
      "2021-09-07 08:43:39,252 - INFO - Using opset <onnx, 11>\n",
      "2021-09-07 08:43:51,608 - INFO - Computed 0 values for constant folding\n",
      "2021-09-07 08:44:27,844 - INFO - Optimizing ONNX model\n",
      "2021-09-07 08:44:38,170 - INFO - After optimization: Cast -123 (311->188), Concat -37 (126->89), Const -1854 (2032->178), Gather +12 (2->14), GlobalAveragePool +50 (0->50), Identity -76 (76->0), ReduceMean -50 (50->0), Shape -37 (112->75), Slice -74 (235->161), Squeeze -198 (223->25), Transpose -12 (61->49), Unsqueeze -361 (435->74)\n",
      "2021-09-07 08:44:39,069 - INFO - \n",
      "2021-09-07 08:44:39,069 - INFO - Successfully converted TensorFlow model ./artifacts/gpt2-model/saved_model/1 to ONNX\n",
      "2021-09-07 08:44:39,069 - INFO - Model inputs: ['attention_mask:0', 'input_ids:0']\n",
      "2021-09-07 08:44:39,070 - INFO - Model outputs: ['logits', 'past_key_values']\n",
      "2021-09-07 08:44:39,070 - INFO - ONNX model is saved at ./artifacts/gpt2-onnx-model/gpt2-model/1/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./artifacts/gpt2-model/saved_model/1 --opset 11  --output ./artifacts/gpt2-onnx-model/gpt2-model/1/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305bc6d",
   "metadata": {},
   "source": [
    "## Deploy GPT2 ONNX Model in Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f63ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ARTIFACT_FOLDER = os.getcwd() + \"/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15512696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tempo.serve.metadata import ModelFramework, ModelDataArgs, ModelDataArg\n",
    "from tempo.serve.model import Model\n",
    "from tempo.serve.pipeline import Pipeline, PipelineModels\n",
    "from tempo.serve.utils import pipeline, predictmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698ac49",
   "metadata": {},
   "source": [
    "#### Define as tempo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49381e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = Model(\n",
    "    name=\"gpt2-model\",\n",
    "    platform=ModelFramework.ONNX,\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-onnx-model\",\n",
    "    uri=\"s3://tempo/gpt2/model\",\n",
    "    description=\"GPT-2 ONNX Triton Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe83780",
   "metadata": {},
   "source": [
    "#### Deploy gpt2 model to docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c15f42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.deploy import deploy_local\n",
    "\n",
    "remote_gpt2_model = deploy_local(gpt2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684edda",
   "metadata": {},
   "source": [
    "#### Send predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34f85261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids:0': array([[1212,  318,  257, 1332]], dtype=int32), 'attention_mask:0': array([[1, 1, 1, 1]], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"This is a test\", return_tensors=\"tf\")\n",
    "attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "gpt2_inputs = {\n",
    "    \"input_ids:0\": input_ids.numpy(),\n",
    "    \"attention_mask:0\": attention_mask\n",
    "}\n",
    "\n",
    "print(gpt2_inputs)\n",
    "\n",
    "gpt2_outputs = remote_gpt2_model.predict(**gpt2_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2500300",
   "metadata": {},
   "source": [
    "#### Print single next token generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "795c55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "# take the best next token probability of the last token of input ( greedy approach)\n",
    "next_token = logits.argmax(axis=2)[0]\n",
    "next_token_str = tokenizer.decode(\n",
    "    next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ").strip()\n",
    "\n",
    "print(next_token_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd1a7e",
   "metadata": {},
   "source": [
    "## Define Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "secure-perth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@pipeline(\n",
    "    name=\"gpt2-transformer\",\n",
    "    uri=\"s3://tempo/gpt2/transformer\",\n",
    "    local_folder=ARTIFACT_FOLDER + \"/gpt2-transformer/\",\n",
    "    models=PipelineModels(gpt2_model=gpt2_model),\n",
    "    description=\"A pipeline to use either an sklearn or xgboost model for Iris classification\",\n",
    ")\n",
    "class GPT2Transformer:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"/mnt/models/\")\n",
    "        except:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(ARTIFACT_FOLDER + \"/gpt2-transformer/\")\n",
    "        \n",
    "    @predictmethod\n",
    "    def predict(self, payload: str) -> str:\n",
    "        count = 0\n",
    "        # TODO: Update to allow this to be passed as parameters\n",
    "        max_gen_len = 10\n",
    "        # TODO: Update to work for multiple sentences\n",
    "        gen_sentence = payload\n",
    "        while count < max_gen_len:\n",
    "            input_ids = self.tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "            attention_mask = np.ones(input_ids.shape.as_list(), dtype=np.int32)\n",
    "\n",
    "            gpt2_inputs = {\n",
    "                \"input_ids:0\": input_ids.numpy(),\n",
    "                \"attention_mask:0\": attention_mask\n",
    "            }\n",
    "\n",
    "            gpt2_outputs = self.models.gpt2_model.predict(**gpt2_inputs)\n",
    "\n",
    "            logits = gpt2_outputs[\"logits\"]\n",
    "\n",
    "            # take the best next token probability of the last token of input ( greedy approach)\n",
    "            next_token = logits.argmax(axis=2)[0]\n",
    "            next_token_str = self.tokenizer.decode(\n",
    "                next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            ).strip()\n",
    "            \n",
    "            gen_sentence += \" \" + next_token_str\n",
    "            count += 1\n",
    "        \n",
    "        return gen_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40315db6",
   "metadata": {},
   "source": [
    "#### Test locally against deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cce2979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_transformer = GPT2Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c140c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_output = gpt2_transformer.predict(\"I love artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9192b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love artificial intelligence , but I 'm not sure if it 's worth\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-collective",
   "metadata": {},
   "source": [
    "## Deploy GPT2 Transformer to Docker and Test\n",
    "\n",
    " * In preparation for running our models we save the Python environment needed for the orchestration to run as defined by a `conda.yaml` in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ambient-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting artifacts/gpt2-transformer/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifacts/gpt2-transformer/conda.yaml\n",
    "name: tempo-gpt2\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.7.10\n",
    "  - pip:\n",
    "    - transformers==4.5.1\n",
    "    - tokenizers==0.10.3\n",
    "    - tensorflow==2.4.1\n",
    "    - dill\n",
    "    - mlops-tempo\n",
    "    - mlserver\n",
    "    - mlserver-tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a86c2",
   "metadata": {},
   "source": [
    "#### Save environment and pipeline artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fixed-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Initialising Insights Manager with Args: ('', 1, 1, 3, 0)\n",
      "WARNING:tempo:Insights Manager not initialised as empty URL provided.\n",
      "INFO:tempo:Saving environment\n",
      "INFO:tempo:Saving tempo model to /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/model.pickle\n",
      "INFO:tempo:Using found conda.yaml\n",
      "INFO:tempo:Creating conda env with: conda env create --name tempo-cb69ce65-9d45-4683-bdfd-592f735994f1 --file /tmp/tmp1vsizgk7.yml\n",
      "INFO:tempo:packing conda environment from tempo-cb69ce65-9d45-4683-bdfd-592f735994f1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packages...\n",
      "Packing environment at '/home/alejandro/miniconda3/envs/tempo-cb69ce65-9d45-4683-bdfd-592f735994f1' to '/home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/environment.tar.gz'\n",
      "[########################################] | 100% Completed | 49.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Removing conda env with: conda remove --name tempo-cb69ce65-9d45-4683-bdfd-592f735994f1 --all --yes\n"
     ]
    }
   ],
   "source": [
    "from tempo.serve.loader import save\n",
    "save(GPT2Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-spanking",
   "metadata": {},
   "source": [
    "#### Deploy locally on Docker\n",
    "\n",
    " * Here we test our models using production images but running locally on Docker. This allows us to ensure the final production deployed model will behave as expected when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "artificial-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_local\n",
    "remote_transformer = deploy_local(gpt2_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "collective-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love artificial intelligence , but I 'm not sure if it 's worth\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_transformer.predict(\"I love artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "found-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Undeploying gpt2-transformer\n",
      "INFO:tempo:Undeploying gpt2-model\n"
     ]
    }
   ],
   "source": [
    "remote_transformer.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-oxygen",
   "metadata": {},
   "source": [
    "## Deploy to Kubernetes\n",
    "\n",
    " * Here we illustrate how to run the final models in \"production\" on Kubernetes by using Tempo to deploy\n",
    " \n",
    "### Prerequisites\n",
    " \n",
    "Create a Kind Kubernetes cluster with Minio and Seldon Core installed using Ansible as described [here](https://tempo.readthedocs.io/en/latest/overview/quickstart.html#kubernetes-cluster-with-seldon-core)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "collect-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): namespaces \"production\" already exists\n",
      "secret/minio-secret configured\n",
      "serviceaccount/tempo-pipeline unchanged\n",
      "role.rbac.authorization.k8s.io/tempo-pipeline unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/tempo-pipeline-rolebinding unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl create ns production\n",
    "!kubectl apply -f k8s/rbac -n production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "meaning-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.examples.minio import create_minio_rclone\n",
    "import os\n",
    "create_minio_rclone(os.getcwd()+\"/rclone.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "collect-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Uploading /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-transformer/ to s3://tempo/gpt2/transformer\n",
      "INFO:tempo:Uploading /home/alejandro/Programming/kubernetes/seldon/tempo/docs/examples/multi-model-gpt2-triton-pipeline/artifacts/gpt2-onnx-model to s3://tempo/gpt2/model\n"
     ]
    }
   ],
   "source": [
    "from tempo.serve.loader import upload\n",
    "upload(gpt2_transformer)\n",
    "upload(gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "civil-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo.serve.metadata import SeldonCoreOptions\n",
    "runtime_options = SeldonCoreOptions(**{\n",
    "        \"remote_options\": {\n",
    "            \"namespace\": \"production\",\n",
    "            \"authSecretName\": \"minio-secret\"\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "binary-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo import deploy_remote\n",
    "remote_gpt2_transformer = deploy_remote(gpt2_transformer, options=runtime_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "psychological-clerk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love artificial intelligence , but I 'm not sure if it 's worth\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_gpt2_transformer.predict(\"I love artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "blind-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tempo:Undeploying gpt2-transformer\n",
      "INFO:tempo:Undeploying gpt2-model\n"
     ]
    }
   ],
   "source": [
    "remote_gpt2_transformer.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309b871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
