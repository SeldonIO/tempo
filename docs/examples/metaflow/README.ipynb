{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f637fb",
   "metadata": {},
   "source": [
    "# End to End ML with Metaflow and Tempo\n",
    "\n",
    "We will train two models and deploy them with tempo within a Metaflow pipeline. To understand the core example see [here](https://tempo.readthedocs.io/en/latest/examples/multi-model/README.html)\n",
    "\n",
    "![archtecture](architecture.png)\n",
    "\n",
    "## MetaFlow Prequisites\n",
    "\n",
    "\n",
    "### Install metaflow locally\n",
    "\n",
    "```\n",
    "pip install metaflow\n",
    "```\n",
    "\n",
    "### Setup Conda-Forge Support\n",
    "\n",
    "The flow will use conda-forge so you need to add that channel to conda.\n",
    "\n",
    "```\n",
    "conda config --add channels conda-forge\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f77af4",
   "metadata": {},
   "source": [
    "## Iris Flow Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc50a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/irisflow.py --environment=conda show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a246ca",
   "metadata": {},
   "source": [
    "## Run Flow locally to deploy to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/irisflow.py --environment=conda run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5376cfc",
   "metadata": {},
   "source": [
    "## Make Predictions with Metaflow Tempo Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea582b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "import numpy as np\n",
    "run = Flow('IrisFlow').latest_run\n",
    "client = run.data.client_model\n",
    "client.predict(np.array([[1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef48e66",
   "metadata": {},
   "source": [
    "## Run Flow on AWS and Deploy to Remote Kubernetes\n",
    "\n",
    "We will now run our flow on AWS Batch and will launch Tempo artifacts onto a remote Kubernetes cluster. \n",
    "\n",
    "### Setup AWS Metaflow Support\n",
    "\n",
    "[Install Metaflow with remote AWS support](https://docs.metaflow.org/metaflow-on-aws/metaflow-on-aws).\n",
    "\n",
    "### Seldon Requirements\n",
    "\n",
    "For deploying to a remote Kubernetes cluster with Seldon Core installed do the following steps:\n",
    "\n",
    "#### Install Seldon Core on your Kubernetes Cluster\n",
    "\n",
    "Create a GKE cluster and install Seldon Core on it using [Ansible to install Seldon Core on a Kubernetes cluster](https://github.com/SeldonIO/ansible-k8s-collection).\n",
    "\n",
    "\n",
    "### K8S Auth from Metaflow\n",
    "\n",
    "To deploy services to our Kubernetes cluster with Seldon Core installed, Metaflow steps that run on AWS Batch and use tempo will need to be able to access K8S API. This step will depend on whether you're using GKE or AWS EKS to run \n",
    "your cluster.\n",
    "\n",
    "#### Option 1. K8S cluster runs on GKE\n",
    "\n",
    "We will need to create two files in the flow src folder:\n",
    "\n",
    "```bash\n",
    "kubeconfig.yaml\n",
    "gsa-key.json\n",
    "```\n",
    "\n",
    "Follow the steps outlined in [GKE server authentication](https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication#environments-without-gcloud).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Option 2. K8S cluster runs on AWS EKS\n",
    "\n",
    "Make note of two AWS IAM role names, for example find them in the IAM console. The names depend on how you deployed Metaflow and EKS in the first place:\n",
    "\n",
    "1. The role used by Metaflow tasks executed on AWS Batch. If you used the default CloudFormation template to deploy Metaflow, it is the role that has `*BatchS3TaskRole*` in its name.\n",
    "\n",
    "2. The role used by EKS nodes. If you used `eksctl` to create your EKS cluster, it is the role that starts with `eksctl-<your-cluster-name>-NodeInstanceRole-*`\n",
    "\n",
    "Now, we need to make sure that AWS Batch role has permissions to access the K8S cluster. For this, add a policy to the AWS Batch task role(1) that has `eks:*` permissions on your EKS cluster (TODO: narrow this down).\n",
    "\n",
    "You'll also need to add a mapping for that role to `aws-auth` ConfigMap in `kube-system` namespace. For more details, see [AWS docs](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html) (under \"To add an IAM user or role to an Amazon EKS cluster\"). In short, you'd need to add this to `mapRoles` section in the aws-auth ConfigMap:\n",
    "```\n",
    "     - rolearn: <batch task role ARN>\n",
    "       username: cluster-admin\n",
    "       groups:\n",
    "         - system:masters\n",
    "```\n",
    "\n",
    "We also need to make sure that the code running in K8S can access S3. For this, add a policy to the EKS node role (2) to allow it to read and write Metaflow S3 buckets.\n",
    "\n",
    "### S3 Authentication\n",
    "Services deployed to Seldon will need to access Metaflow S3 bucket to download trained models. The exact configuration will depend on whether you're using GKE or AWS EKS to run your cluster.\n",
    "\n",
    "From the base templates provided below, create your `k8s/s3_secret.yaml`.\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: s3-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  RCLONE_CONFIG_S3_TYPE: s3\n",
    "  RCLONE_CONFIG_S3_PROVIDER: aws\n",
    "  RCLONE_CONFIG_S3_BUCKET_REGION: <region>\n",
    "  <...cloud-dependent s3 auth settings (see below)>\n",
    "```\n",
    "\n",
    "For GKE, to access S3 we'll need to add the following variables to use key/secret auth:\n",
    "```yaml\n",
    "  RCLONE_CONFIG_S3_ENV_AUTH: \"false\"\n",
    "  RCLONE_CONFIG_S3_ACCESS_KEY_ID: <key>\n",
    "  RCLONE_CONFIG_S3_SECRET_ACCESS_KEY: <secret>\n",
    "```\n",
    "\n",
    "For AWS EKS, we'll use the instance role assigned to the node, we'll only need to set one env variable:\n",
    "```yaml\n",
    "RCLONE_CONFIG_S3_ENV_AUTH: \"true\"\n",
    "```\n",
    "\n",
    "We provide two templates to use in the `k8s` folder:\n",
    "\n",
    "```\n",
    "s3_secret.yaml.tmpl.aws\n",
    "s3_secret.yaml.tmpl.gke\n",
    "```\n",
    "\n",
    "Use one to create the file `s3_secret.yaml` in the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09175b",
   "metadata": {},
   "source": [
    "## Setup RBAC and Secret on Kubernetes Cluster\n",
    "\n",
    "These steps assume you have authenticated to your cluster with kubectl configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create ns production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840694c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f k8s/tempo-pipeline-rbac.yaml -n production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68dc155",
   "metadata": {},
   "source": [
    "Create a Secret from the `k8s/s3_secret.yaml.tmpl` file by adding your AWS Key that can read from S3 and saving as `k8s/s3_secret.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f k8s/s3_secret.yaml -n production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e772b",
   "metadata": {},
   "source": [
    "## Run Metaflow on AWS Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f43482",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/irisflow.py \\\n",
    "    --environment=conda \\\n",
    "    --with batch:image=seldonio/seldon-core-s2i-python37-ubi8:1.10.0-dev \\\n",
    "    run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3e32c",
   "metadata": {},
   "source": [
    "## Make Predictions with Metaflow Tempo Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "run = Flow('IrisFlow').latest_run\n",
    "client = run.data.client_model\n",
    "import numpy as np\n",
    "client.predict(np.array([[1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cddc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
